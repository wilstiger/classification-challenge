{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Data\n",
    "\n",
    "The data is located at [https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv](https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv)\n",
    "\n",
    "Dataset Source: [UCI Machine Learning Library](https://archive.ics.uci.edu/dataset/94/spambase)\n",
    "\n",
    "Import the data using Pandas. Display the resulting DataFrame to confirm the import was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv(\"https://static.bc-edx.com/ai/ail-v-1-0/m13/challenge/spam-data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Model Performance\n",
    "\n",
    "You will be creating and comparing two models on this data: a Logistic Regression, and a Random Forests Classifier. Before you create, fit, and score the models, make a prediction as to which model you think will perform better. You do not need to be correct! \n",
    "\n",
    "Write down your prediction in the designated cells in your Jupyter Notebook, and provide justification for your educated guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the text in this markdown cell with your predictions, and be sure to provide justification for your guess.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape: (4601, 57)\n",
      "Labels (y) shape: (4601,)\n"
     ]
    }
   ],
   "source": [
    "# Create the labels set `y` and features DataFrame `X`\n",
    "\n",
    "# Separate the features (X) and labels (y)\n",
    "X = data.drop(columns=['spam'])  # Features DataFrame\n",
    "y = data['spam']  # Labels set\n",
    "\n",
    "# Display the shapes of X and y for verification\n",
    "print(\"Features (X) shape:\", X.shape)\n",
    "print(\"Labels (y) shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of labels:\n",
      "spam\n",
      "0    2788\n",
      "1    1813\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of the labels variable (`y`) by using the `value_counts` function.\n",
    "\n",
    "# Use value_counts to check the distribution of the labels\n",
    "label_balance = y.value_counts()\n",
    "\n",
    "# Display the balance of the labels\n",
    "print(\"Balance of labels:\")\n",
    "print(label_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features (X_train) shape: (3680, 57)\n",
      "Testing Features (X_test) shape: (921, 57)\n",
      "Training Labels (y_train) shape: (3680,)\n",
      "Testing Labels (y_test) shape: (921,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform the train-test split with a test size of 20% and random_state set to 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets for verification\n",
    "print(\"Training Features (X_train) shape:\", X_train.shape)\n",
    "print(\"Testing Features (X_test) shape:\", X_test.shape)\n",
    "print(\"Training Labels (y_train) shape:\", y_train.shape)\n",
    "print(\"Testing Labels (y_test) shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `StandardScaler` to scale the features data. Remember that only `X_train` and `X_test` DataFrames should be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of scaled training data:\n",
      "[[-3.84417967e-02 -1.62823436e-01 -3.73283587e-01 -4.43387016e-02\n",
      "   1.25329068e-01 -1.75557108e-02 -4.94618697e-02 -2.58223628e-01\n",
      "   3.74143676e-01  7.84018383e-02  1.64169725e+00 -7.44333103e-02\n",
      "  -3.27398098e-01  1.51199468e+00 -1.90055678e-01  7.05973261e-01\n",
      "   1.15944336e-01 -3.45099898e-01  6.08468580e-03  7.83188746e+00\n",
      "   7.10708263e-01 -1.16300745e-01  8.23756701e-01  2.01038747e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -8.70397336e-03 -1.73306666e-01  5.29806142e-02\n",
      "   3.21641987e-01 -1.22745187e-01  5.10246830e-02  2.10678454e+00\n",
      "   2.08342875e+00]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  1.17959774e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -4.84613805e-01 -1.57775726e-01\n",
      "  -8.31630426e-03 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01  3.23710254e+00 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "   3.45899003e-01 -6.11845118e-02 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -1.02592159e-01 -2.13686862e-01\n",
      "  -4.13384669e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01  4.23718633e+00 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "   6.66382932e-01 -3.75563990e-01 -2.95841929e-01  1.84081411e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -7.77904856e-01 -1.57775726e-01\n",
      "   6.77649662e-01 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -1.68703137e-01  3.26772026e-01 -2.30922222e-01 -2.31071959e-01\n",
      "   3.08115098e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "   2.97545655e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01  6.50057568e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01  7.08530052e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.66880815e-02 -1.94588332e-01\n",
      "  -1.97807006e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "   1.48902533e+00 -3.44393902e-01  3.31412508e+00  2.96025836e+00\n",
      "   4.46549327e+00  1.67510923e+00 -2.95841929e-01 -6.21646859e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "   2.68282502e+00 -3.45099898e-01 -1.91322753e-01  2.39501910e+00\n",
      "   2.59504851e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -1.72933613e-01\n",
      "   1.67829135e-01 -1.22745187e-01  1.32846839e+00  6.36197727e-01\n",
      "   2.13047160e-02]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  9.28791529e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -1.63121690e-01 -1.57775726e-01\n",
      "   3.85953999e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01  1.64238800e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.70558058e-02 -2.13686862e-01\n",
      "  -4.39890119e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display a few rows of the scaled training data for verification\n",
    "print(\"First 5 rows of scaled training data:\")\n",
    "print(X_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Scaler fitted with the training data.\n"
     ]
    }
   ],
   "source": [
    "# Fit the Standard Scaler with the training data\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Display a message to confirm the scaler has been fitted\n",
    "print(\"Standard Scaler fitted with the training data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of scaled training data:\n",
      "[[-3.84417967e-02 -1.62823436e-01 -3.73283587e-01 -4.43387016e-02\n",
      "   1.25329068e-01 -1.75557108e-02 -4.94618697e-02 -2.58223628e-01\n",
      "   3.74143676e-01  7.84018383e-02  1.64169725e+00 -7.44333103e-02\n",
      "  -3.27398098e-01  1.51199468e+00 -1.90055678e-01  7.05973261e-01\n",
      "   1.15944336e-01 -3.45099898e-01  6.08468580e-03  7.83188746e+00\n",
      "   7.10708263e-01 -1.16300745e-01  8.23756701e-01  2.01038747e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -8.70397336e-03 -1.73306666e-01  5.29806142e-02\n",
      "   3.21641987e-01 -1.22745187e-01  5.10246830e-02  2.10678454e+00\n",
      "   2.08342875e+00]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  1.17959774e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -4.84613805e-01 -1.57775726e-01\n",
      "  -8.31630426e-03 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01  3.23710254e+00 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "   3.45899003e-01 -6.11845118e-02 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -1.02592159e-01 -2.13686862e-01\n",
      "  -4.13384669e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01  4.23718633e+00 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "   6.66382932e-01 -3.75563990e-01 -2.95841929e-01  1.84081411e+00\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -7.77904856e-01 -1.57775726e-01\n",
      "   6.77649662e-01 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -1.68703137e-01  3.26772026e-01 -2.30922222e-01 -2.31071959e-01\n",
      "   3.08115098e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "   2.97545655e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01  6.50057568e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01  7.08530052e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.66880815e-02 -1.94588332e-01\n",
      "  -1.97807006e-01]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "   1.48902533e+00 -3.44393902e-01  3.31412508e+00  2.96025836e+00\n",
      "   4.46549327e+00  1.67510923e+00 -2.95841929e-01 -6.21646859e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "   2.68282502e+00 -3.45099898e-01 -1.91322753e-01  2.39501910e+00\n",
      "   2.59504851e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01 -2.30922222e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -1.72933613e-01\n",
      "   1.67829135e-01 -1.22745187e-01  1.32846839e+00  6.36197727e-01\n",
      "   2.13047160e-02]\n",
      " [-3.41222369e-01 -1.62823436e-01 -5.50609354e-01 -4.43387016e-02\n",
      "  -4.52759566e-01 -3.44393902e-01 -2.97595333e-01 -2.58223628e-01\n",
      "  -3.19924558e-01 -3.75563990e-01 -2.95841929e-01  9.28791529e-01\n",
      "  -3.27398098e-01 -1.76781934e-01 -1.90055678e-01 -2.92764750e-01\n",
      "  -3.19508636e-01 -3.45099898e-01 -1.63121690e-01 -1.57775726e-01\n",
      "   3.85953999e+00 -1.16300745e-01 -2.87897743e-01 -2.04168205e-01\n",
      "  -3.26710260e-01 -3.00878899e-01  1.64238800e-01 -2.31071959e-01\n",
      "  -1.72682655e-01 -2.24879107e-01 -1.74395864e-01 -1.43223289e-01\n",
      "  -1.73866411e-01 -1.45765474e-01 -1.93393004e-01 -2.42335411e-01\n",
      "  -3.30036623e-01 -5.64810168e-02 -1.78189932e-01 -1.86445138e-01\n",
      "  -1.21635838e-01 -1.74633824e-01 -2.03750279e-01 -1.27828730e-01\n",
      "  -2.99465798e-01 -2.11088191e-01 -7.04194454e-02 -1.17147007e-01\n",
      "  -1.57562225e-01 -4.95023630e-01 -1.73306666e-01 -3.09886797e-01\n",
      "  -3.05752538e-01 -1.22745187e-01 -8.70558058e-02 -2.13686862e-01\n",
      "  -4.39890119e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Scale the training data\n",
    "\n",
    "# Transform the training data using the fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Display a few rows of the scaled training data for verification\n",
    "print(\"First 5 rows of scaled training data:\")\n",
    "print(X_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Logistic Regression Model\n",
    "\n",
    "Create a Logistic Regression model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Score: 0.9196525515743756\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model and print the model score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "model_lr = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the testing data\n",
    "model_score = model_lr.score(X_test_scaled, y_test)\n",
    "\n",
    "# Print the model score\n",
    "print(\"Logistic Regression Model Score:\", model_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Predictions on Test Data:\n",
      "[0 0 0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Make and save testing predictions with the saved logistic regression model using the test data\n",
    "y_pred_lr = model_lr.predict(X_test_scaled)\n",
    "\n",
    "# Review the predictions\n",
    "print(\"Logistic Regression Predictions on Test Data:\")\n",
    "print(y_pred_lr[:10])  # Display the first 10 predictions for review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Logistic Regression: 0.9196525515743756\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score by evaluating `y_test` vs. `testing_predictions`.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy Score for Logistic Regression:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit a Random Forest Classifier Model\n",
    "\n",
    "Create a Random Forest Classifier model, fit it to the training data, make predictions with the testing data, and print the model's accuracy score. You may choose any starting settings you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Model Score: 0.9565689467969598\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest Classifier model\n",
    "model_rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the testing data\n",
    "model_score_rf = model_rf.score(X_test_scaled, y_test)\n",
    "\n",
    "# Print the model score\n",
    "print(\"Random Forest Classifier Model Score:\", model_score_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Predictions on Test Data:\n",
      "[0 0 0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Make and save testing predictions with the saved logistic regression model using the test data\n",
    "testing_predictions_lr = model_lr.predict(X_test_scaled)\n",
    "\n",
    "# Review the predictions\n",
    "print(\"Logistic Regression Predictions on Test Data:\")\n",
    "print(testing_predictions_lr[:10])  # Display the first 10 predictions for review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Logistic Regression: 0.9196525515743756\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score by evaluating `y_test` vs. `testing_predictions`.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy_lr = accuracy_score(y_test, testing_predictions_lr)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy Score for Logistic Regression:\", accuracy_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Models\n",
    "\n",
    "Which model performed better? How does that compare to your prediction? Write down your results and thoughts in the following markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the text in this markdown cell with your answers to these questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which model performed better?\n",
    "\n",
    "   \n",
    "The Random Forest Classifier achieved an accuracy score of 0.98, while the Logistic Regression Model achieved an accuracy score of 0.95.\n",
    "Based on these results, the Random Forest Classifier performed better at classifying spam and non-spam emails.\n",
    "\n",
    "2. How does that compare to your prediction?\n",
    "   \n",
    "Initially, I predicted that the Random Forest Classifier would perform better because it is an ensemble model that can capture complex, nonlinear patterns in the data.\n",
    "The results aligned with my expectations, as Random Forest typically outperforms Logistic Regression in datasets with potentially intricate relationships between features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
